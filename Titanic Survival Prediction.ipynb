{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training and testing files. I'm using the titanic dataset from Kaggle for Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the test set does not have the target label 'survived' as Kaggle didn't release the test set with labels. <br>\n",
    "- So, for our purpose, I'm sampling test set randomly from the train files. <br>\n",
    "10% of trainset will be test.\n",
    "- Remaining train set is split into train and validation sets in the ratio 9:1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train.drop('Survived', axis=1), train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=111)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, stratify=y_train, random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>720.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>585.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>441.098611</td>\n",
       "      <td>2.304167</td>\n",
       "      <td>29.839043</td>\n",
       "      <td>0.536111</td>\n",
       "      <td>0.384722</td>\n",
       "      <td>32.834183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>256.925120</td>\n",
       "      <td>0.837563</td>\n",
       "      <td>14.700694</td>\n",
       "      <td>1.113241</td>\n",
       "      <td>0.815729</td>\n",
       "      <td>49.459847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>219.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>663.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.620825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
       "count   720.000000  720.000000  585.000000  720.000000  720.000000  720.000000\n",
       "mean    441.098611    2.304167   29.839043    0.536111    0.384722   32.834183\n",
       "std     256.925120    0.837563   14.700694    1.113241    0.815729   49.459847\n",
       "min       1.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%     219.750000    2.000000   20.500000    0.000000    0.000000    7.895800\n",
       "50%     438.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%     663.250000    3.000000   39.000000    1.000000    0.000000   31.620825\n",
       "max     891.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1)\n",
    "\n",
    "x_train = encoder.fit_transform(x_train)\n",
    "x_val = encoder.transform(x_val)\n",
    "x_test = encoder.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll optimize the hyperparameters one-by-one, keeping the other hyperparameters to their default values until we find an optimized value of those hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Maximum number of iterations, max_iter hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_iter = 100:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_iter = 200:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_iter = 500:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_iter = 1000:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_iter = 10000:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n"
     ]
    }
   ],
   "source": [
    "max_iter = [100, 200, 500, 1000, 10000]\n",
    "\n",
    "for mi in max_iter:\n",
    "    softmax_reg = LogisticRegression(penalty='l2', C=1, solver='lbfgs', max_iter=mi)\n",
    "    softmax_reg.fit(x_train, y_train)\n",
    "    acc_train = softmax_reg.score(x_train, y_train)\n",
    "    acc_val = softmax_reg.score(x_val, y_val)\n",
    "    print(f'For max_iter = {mi}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the model converges in 100 iterations as neither training nor validation accuracy improves after 100 iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Regularization Coefficient (actually it's inverse), C hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 1e-05:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.0001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.001:\n",
      "\t Training accuracy: 0.7138888888888889 \t\t Validation accuracy: 0.6419753086419753\n",
      "For C = 0.01:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For C = 0.1:\n",
      "\t Training accuracy: 0.8263888888888888 \t\t Validation accuracy: 0.7901234567901234\n",
      "For C = 1:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For C = 10:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For C = 100:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n",
      "For C = 1000:\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7901234567901234\n"
     ]
    }
   ],
   "source": [
    "C = [0.00001, 0.0001,0.001,0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for c in C:\n",
    "    softmax_reg = LogisticRegression(penalty='l2', C=c, solver='lbfgs', max_iter=100)\n",
    "    softmax_reg.fit(x_train, y_train)\n",
    "    acc_train = softmax_reg.score(x_train, y_train)\n",
    "    acc_val = softmax_reg.score(x_val, y_val)\n",
    "    print(f'For C = {c}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for C = 0.01\n",
    "- For C < 0.01: both the training and validation accuracies are less as compared to that of C = 0.01, indicating that the model underfits the training data. <br>\n",
    "- For C > 0.01: the training and validation accuracies don't improve much, in fact the validation accuracy decreases, indicating that the value of C doesn't have any considerable impact on the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the solver hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For solver = liblinear:\n",
      "\t Training accuracy: 0.8 \t\t Validation accuracy: 0.8024691358024691\n",
      "For solver = newton-cg:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For solver = newton-cholesky:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For solver = sag:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For solver = saga:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For solver = lbfgs:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "solver = ['liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs']\n",
    "\n",
    "for solv in solver:\n",
    "    softmax_reg = LogisticRegression(penalty='l2', C=0.01, solver=solv, max_iter=100)\n",
    "    softmax_reg.fit(x_train, y_train)\n",
    "    acc_train = softmax_reg.score(x_train, y_train)\n",
    "    acc_val = softmax_reg.score(x_val, y_val)\n",
    "    print(f'For solver = {solv}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The solver hyperparameter doesn't have any impact on the model's performance as both the training and validation accuracies are same for all the solvers. We'll select the \"lbfgs\" solver as it's convergence is faster as compared to other solvers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the norm of regularization, penalty hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For penalty = l1:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For penalty = l2:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For penalty = elasticnet:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "\n",
    "for pen in penalty:\n",
    "    softmax_reg = LogisticRegression(penalty=pen, C=0.01, solver='saga', max_iter=100, l1_ratio=0.5)\n",
    "    softmax_reg.fit(x_train, y_train)\n",
    "    acc_train = softmax_reg.score(x_train, y_train)\n",
    "    acc_val = softmax_reg.score(x_val, y_val)\n",
    "    print(f'For penalty = {pen}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The l1 and elasticnet penalties have more impact on the model weights and reduce it's capability to fit the data properly which is evident from the low training and validation accuracies. <br>\n",
    "We'll select l2 penalty term as it gives the best training and validation accuracies out of the 3 penalty terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have got the optimal values of required hyperparameters, so we'll just train the model for different number of max iterations to find if it impacts the convergence point of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_iter = 100:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 200:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 500:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 1000:\n",
      "\t Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "max_iter = [100, 200, 500, 1000]\n",
    "\n",
    "for mi in max_iter:\n",
    "    softmax_reg = LogisticRegression(penalty='l2', C=0.01, solver='lbfgs', max_iter=mi)\n",
    "    softmax_reg.fit(x_train, y_train)\n",
    "    acc_train = softmax_reg.score(x_train, y_train)\n",
    "    acc_val = softmax_reg.score(x_val, y_val)\n",
    "    print(f'For max_iter = {mi}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model converges to same point for all max_iter values. So, we'll need only 100 iterations for the model to converge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Logistic Regression model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.01)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(penalty='l2', C=0.01, solver='lbfgs', max_iter=100)\n",
    "softmax_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8166666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "Training precision score: 0.8196363636363637 \t\t Validation precision score: 0.7927718832891246\n",
      "Training recall score: 0.7869173521347435 \t\t Validation recall score: 0.7848387096774194\n",
      "Training f1 score: 0.7970202296336796 \t\t Validation f1 score: 0.788235294117647\n"
     ]
    }
   ],
   "source": [
    "acc_train = softmax_reg.score(x_train, y_train)\n",
    "acc_val = softmax_reg.score(x_val, y_val)\n",
    "\n",
    "y_train_pred = softmax_reg.predict(x_train)\n",
    "y_val_pred = softmax_reg.predict(x_val)\n",
    "\n",
    "prec_train = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "prec_val = precision_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "rec_train = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "rec_val = recall_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "print(f'Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')\n",
    "print(f'Training precision score: {prec_train} \\t\\t Validation precision score: {prec_val}')\n",
    "print(f'Training recall score: {rec_train} \\t\\t Validation recall score: {rec_val}')\n",
    "print(f'Training f1 score: {f1_train} \\t\\t Validation f1 score: {f1_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[406,  38],\n",
       "       [ 94, 182]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43,  7],\n",
       "       [ 9, 22]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_val, y_val_pred)\n",
    "conf_mx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Maximum number of iterations, max_iter hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_iter = 1000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 10000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 40000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 50000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 60000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 80000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_iter = 100000:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "max_iter = [1000, 10000, 40000, 50000, 60000, 80000, 100000]\n",
    "\n",
    "for mi in max_iter:\n",
    "    svm_clf = SVC(C=1, kernel='linear', max_iter=mi)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For max_iter = {mi}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the model converges in 1000 iterations as neither training nor validation accuracy improves after 1000 iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Regularization Coefficient (actually it's inverse), C hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 0.0001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.001:\n",
      "\t Training accuracy: 0.7333333333333333 \t\t Validation accuracy: 0.6296296296296297\n",
      "For C = 0.01:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For C = 0.1:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For C = 1:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For C = 10:\n",
      "\t Training accuracy: 0.7291666666666666 \t\t Validation accuracy: 0.6419753086419753\n",
      "For C = 100:\n",
      "\t Training accuracy: 0.5847222222222223 \t\t Validation accuracy: 0.43209876543209874\n",
      "For C = 1000:\n",
      "\t Training accuracy: 0.6291666666666667 \t\t Validation accuracy: 0.4074074074074074\n"
     ]
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for c in C:\n",
    "    svm_clf = SVC(C=c, kernel='linear', max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For C = {c}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for C = 0.01, 0.1 and 1\n",
    "- For C < 0.01: both the training and validation accuracies are less as compared to that of C = 0.01, indicating that the model underfits the training data. <br>\n",
    "- For C > 1: the training and validation accuracies start degrading, indicating that the model is imposing heavy regularization on the model's weights and thus the model fails to converge and underfits the training data.\n",
    "\n",
    "We'll choose C = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Linear SVM model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(C=0.01, kernel='linear', max_iter=1000)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "acc_train = svm_clf.score(x_train, y_train)\n",
    "acc_val = svm_clf.score(x_val, y_val)\n",
    "print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Maximum number of iterations, max_iter hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_iter = 1000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 10000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 40000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 50000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 60000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 80000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 100000:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "max_iter = [1000, 10000, 40000, 50000, 60000, 80000, 100000]\n",
    "\n",
    "for mi in max_iter:\n",
    "    svm_clf = SVC(C=1, kernel='poly', degree=3, gamma='scale', coef0=0, max_iter=mi)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For max_iter = {mi}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the model converges in 1000 iterations as neither training nor validation accuracy improves after 1000 iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Regularization Coefficient (actually it's inverse), C hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 0.0001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.01:\n",
      "\t Training accuracy: 0.6416666666666667 \t\t Validation accuracy: 0.6296296296296297\n",
      "For C = 0.1:\n",
      "\t Training accuracy: 0.775 \t\t Validation accuracy: 0.7037037037037037\n",
      "For C = 1:\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.8024691358024691\n",
      "For C = 10:\n",
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "For C = 100:\n",
      "\t Training accuracy: 0.9013888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For C = 1000:\n",
      "\t Training accuracy: 0.7 \t\t Validation accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for c in C:\n",
    "    svm_clf = SVC(C=c, kernel='poly', degree=3, gamma='scale', coef0=0, max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For C = {c}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for C = 10\n",
    "- For C < 10: both the training and validation accuracies are less as compared to that of C = 10, indicating that the model underfits the training data. <br>\n",
    "- For C > 1: the training and validation accuracies start degrading, indicating that the model is imposing heavy regularization on the model's weights and thus the model fails to converge and underfits the training data.\n",
    "\n",
    "We'll choose C = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the degree hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For degree = 1:\n",
      "\t Training accuracy: 0.7902777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For degree = 2:\n",
      "\t Training accuracy: 0.8347222222222223 \t\t Validation accuracy: 0.8024691358024691\n",
      "For degree = 3:\n",
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "For degree = 4:\n",
      "\t Training accuracy: 0.9333333333333333 \t\t Validation accuracy: 0.8024691358024691\n",
      "For degree = 5:\n",
      "\t Training accuracy: 0.9347222222222222 \t\t Validation accuracy: 0.7777777777777778\n",
      "For degree = 6:\n",
      "\t Training accuracy: 0.9138888888888889 \t\t Validation accuracy: 0.7160493827160493\n",
      "For degree = 7:\n",
      "\t Training accuracy: 0.9083333333333333 \t\t Validation accuracy: 0.7777777777777778\n",
      "For degree = 8:\n",
      "\t Training accuracy: 0.8944444444444445 \t\t Validation accuracy: 0.7530864197530864\n",
      "For degree = 9:\n",
      "\t Training accuracy: 0.8833333333333333 \t\t Validation accuracy: 0.7777777777777778\n",
      "For degree = 10:\n",
      "\t Training accuracy: 0.8763888888888889 \t\t Validation accuracy: 0.7283950617283951\n"
     ]
    }
   ],
   "source": [
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for deg in degrees:\n",
    "    svm_clf = SVC(C=10, kernel='poly', degree=deg, gamma='scale', coef0=0, max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For degree = {deg}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that with increase in degree of the model, the training and validation accuracies increase upto 3rd degree and after that the training accuracy increases until degree 5 but the validation accuracy decreases during this interval, indicating that the model overfits on the training data. Beyond degree 5, both training and validation accuracies decrease as compared to that of degree 3.\n",
    "\n",
    "So, we choose degree = 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the gamma hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gamma = scale:\n",
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "For gamma = auto:\n",
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "For gamma = 0.01:\n",
      "\t Training accuracy: 0.6583333333333333 \t\t Validation accuracy: 0.6296296296296297\n",
      "For gamma = 0.1:\n",
      "\t Training accuracy: 0.9138888888888889 \t\t Validation accuracy: 0.8024691358024691\n",
      "For gamma = 1:\n",
      "\t Training accuracy: 0.6791666666666667 \t\t Validation accuracy: 0.6790123456790124\n",
      "For gamma = 10:\n",
      "\t Training accuracy: 0.5819444444444445 \t\t Validation accuracy: 0.6049382716049383\n",
      "For gamma = 100:\n",
      "\t Training accuracy: 0.5708333333333333 \t\t Validation accuracy: 0.6049382716049383\n"
     ]
    }
   ],
   "source": [
    "gamma = ['scale', 'auto', 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for g in gamma:\n",
    "    svm_clf = SVC(C=10, kernel='poly', degree=3, gamma=g, coef0=0, max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For gamma = {g}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model gives best training and validation accuracies when gamma is set to \"auto\" or \"scale\". \n",
    "- gamma='scale' -> kernel coefficient = 1 / (n_features * x_train.var()\n",
    "- gamma=‘auto’ -> kernel coefficient = 1 / n_features\n",
    "\n",
    "As for gamma = \"auto\" or \"scale\", the kernel coefficient is computed using the number of features (and variance of training data in case of \"scale\"), the model fits the data better as compared to other random values. <br><br>\n",
    "So, we choose gamma=\"scale\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the coef0 hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For coef0 = 0:\n",
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "For coef0 = 0.01:\n",
      "\t Training accuracy: 0.9152777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For coef0 = 0.1:\n",
      "\t Training accuracy: 0.925 \t\t Validation accuracy: 0.7901234567901234\n",
      "For coef0 = 1:\n",
      "\t Training accuracy: 0.9263888888888889 \t\t Validation accuracy: 0.7654320987654321\n",
      "For coef0 = 10:\n",
      "\t Training accuracy: 0.6152777777777778 \t\t Validation accuracy: 0.6666666666666666\n",
      "For coef0 = 100:\n",
      "\t Training accuracy: 0.6819444444444445 \t\t Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "coef = [0, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for c in coef:\n",
    "    svm_clf = SVC(C=10, kernel='poly', degree=3, gamma='scale', coef0=c, max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For coef0 = {c}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for coef0 = 0. For coef0 between 0 and 1, the training accuracy increases but the validation accuracy decreases, indicating overfitting. For coef0 > 1, both training and validation accuracies increase or decrease together. <br>\n",
    "So, we choose coef0 = 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final polynomial SVM model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(C=10, kernel='poly', degree=3, gamma='scale', coef0=0, max_iter=1000)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "acc_train = svm_clf.score(x_train, y_train)\n",
    "acc_val = svm_clf.score(x_val, y_val)\n",
    "print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Maximum number of iterations, max_iter hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_iter = 1000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 10000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 40000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 50000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 60000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 80000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_iter = 100000:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "max_iter = [1000, 10000, 40000, 50000, 60000, 80000, 100000]\n",
    "\n",
    "for mi in max_iter:\n",
    "    svm_clf = SVC(C=1, kernel='rbf', gamma='scale', max_iter=mi)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For max_iter = {mi}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the model converges in 1000 iterations as neither training nor validation accuracy improves after 1000 iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Regularization Coefficient (actually it's inverse), C hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 0.0001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.001:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.01:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 0.1:\n",
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For C = 1:\n",
      "\t Training accuracy: 0.8527777777777777 \t\t Validation accuracy: 0.8024691358024691\n",
      "For C = 10:\n",
      "\t Training accuracy: 0.9319444444444445 \t\t Validation accuracy: 0.7654320987654321\n",
      "For C = 100:\n",
      "\t Training accuracy: 0.9625 \t\t Validation accuracy: 0.6419753086419753\n",
      "For C = 1000:\n",
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For C = 10000:\n",
      "\t Training accuracy: 0.7666666666666667 \t\t Validation accuracy: 0.691358024691358\n"
     ]
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "for c in C:\n",
    "    svm_clf = SVC(C=c, kernel='rbf', gamma='scale', max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For C = {c}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for C = 0.1 and 1\n",
    "- For C < 0.1: both the training and validation accuracies are less as compared to that of C = 0.1, indicating that the model underfits the training data. <br>\n",
    "- For 1 < C <= 100: the training accuracy improves but the validation accuracy decreases indicating that the overfits on the training data and doesn't generalize well on the validation set \n",
    "- For C > 100: both the training and validation accuracies start degrading, indicating that the model is imposing heavy regularization on the model's weights and thus the model fails to converge and underfits the training data.\n",
    "\n",
    "We'll choose C = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the gamma hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gamma = scale:\n",
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For gamma = auto:\n",
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For gamma = 0.01:\n",
      "\t Training accuracy: 0.7944444444444444 \t\t Validation accuracy: 0.7407407407407407\n",
      "For gamma = 0.1:\n",
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For gamma = 1:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For gamma = 10:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n",
      "For gamma = 100:\n",
      "\t Training accuracy: 0.6166666666666667 \t\t Validation accuracy: 0.6172839506172839\n"
     ]
    }
   ],
   "source": [
    "gamma = ['scale', 'auto', 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for g in gamma:\n",
    "    svm_clf = SVC(C=0.1, kernel='rbf', gamma=g, max_iter=1000)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    acc_train = svm_clf.score(x_train, y_train)\n",
    "    acc_val = svm_clf.score(x_val, y_val)\n",
    "    print(f'For gamma = {g}:')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model gives best training and validation accuracies when gamma is set to \"auto\" or \"scale\". \n",
    "- gamma='scale' -> kernel coefficient = 1 / (n_features * x_train.var()\n",
    "- gamma=‘auto’ -> kernel coefficient = 1 / n_features\n",
    "\n",
    "As for gamma = \"auto\" or \"scale\", the kernel coefficient is computed using the number of features (and variance of training data in case of \"scale\"), the model fits the data better as compared to other random values. <br><br>\n",
    "So, we choose gamma=\"scale\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final RBF Kernel SVM model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: 0.8416666666666667 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(C=0.1, kernel='rbf', gamma='scale', max_iter=1000, probability=True, random_state=4200)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "acc_train = svm_clf.score(x_train, y_train)\n",
    "acc_val = svm_clf.score(x_val, y_val)\n",
    "print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of Linear, Polynomial and RBF Kernel SVM models, the Polynomial SVM model performs best so we choose Polynomial SVM as the final SVM model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(C=10, kernel='poly', degree=3, gamma='scale', coef0=0, max_iter=1000, probability=True, random_state=1331)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "acc_train = svm_clf.score(x_train, y_train)\n",
    "acc_val = svm_clf.score(x_val, y_val)\n",
    "print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9125 \t\t Validation accuracy: 0.8148148148148148\n",
      "Training precision score: 0.9115393283750282 \t\t Validation precision score: 0.8055555555555556\n",
      "Training recall score: 0.9023207990599296 \t\t Validation recall score: 0.8193548387096774\n",
      "Training f1 score: 0.9064570611926724 \t\t Validation f1 score: 0.8091123330714847\n"
     ]
    }
   ],
   "source": [
    "acc_train = svm_clf.score(x_train, y_train)\n",
    "acc_val = svm_clf.score(x_val, y_val)\n",
    "\n",
    "y_train_pred = svm_clf.predict(x_train)\n",
    "y_val_pred = svm_clf.predict(x_val)\n",
    "\n",
    "prec_train = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "prec_val = precision_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "rec_train = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "rec_val = recall_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "print(f'Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')\n",
    "print(f'Training precision score: {prec_train} \\t\\t Validation precision score: {prec_val}')\n",
    "print(f'Training recall score: {rec_train} \\t\\t Validation recall score: {rec_val}')\n",
    "print(f'Training f1 score: {f1_train} \\t\\t Validation f1 score: {f1_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[420,  24],\n",
       "       [ 39, 237]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 10],\n",
       "       [ 5, 26]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_val, y_val_pred)\n",
    "conf_mx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the number of decision trees in the forest, n_estimators hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators = 1\n",
      "\t Training accuracy: 0.8819444444444444 \t\t Validation accuracy: 0.7037037037037037\n",
      "For n_estimators = 2\n",
      "\t Training accuracy: 0.8986111111111111 \t\t Validation accuracy: 0.7160493827160493\n",
      "For n_estimators = 3\n",
      "\t Training accuracy: 0.9569444444444445 \t\t Validation accuracy: 0.7160493827160493\n",
      "For n_estimators = 4\n",
      "\t Training accuracy: 0.9541666666666667 \t\t Validation accuracy: 0.7530864197530864\n",
      "For n_estimators = 5\n",
      "\t Training accuracy: 0.9777777777777777 \t\t Validation accuracy: 0.7530864197530864\n",
      "For n_estimators = 6\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For n_estimators = 7\n",
      "\t Training accuracy: 0.9888888888888889 \t\t Validation accuracy: 0.7037037037037037\n",
      "For n_estimators = 8\n",
      "\t Training accuracy: 0.9833333333333333 \t\t Validation accuracy: 0.691358024691358\n",
      "For n_estimators = 9\n",
      "\t Training accuracy: 0.9930555555555556 \t\t Validation accuracy: 0.6666666666666666\n",
      "For n_estimators = 10\n",
      "\t Training accuracy: 0.9930555555555556 \t\t Validation accuracy: 0.691358024691358\n"
     ]
    }
   ],
   "source": [
    "n_est = [i+1 for i in range(10) ]\n",
    "\n",
    "for est in n_est:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=est, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For n_estimators = {est}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the training and validation accuracies increase with the increase in number of decision trees in the forest till n_estimators = 6 and after that the training accuracy increases but the validation accuracy starts decreasing which means that the model is overfitting on the training data. <br>\n",
    "So, we choose n_estimators = 6."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the impurity measure, criterion hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For criterion = gini\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For criterion = entropy\n",
      "\t Training accuracy: 0.9736111111111111 \t\t Validation accuracy: 0.6172839506172839\n",
      "For criterion = log_loss\n",
      "\t Training accuracy: 0.9736111111111111 \t\t Validation accuracy: 0.6172839506172839\n"
     ]
    }
   ],
   "source": [
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "for crit in criterion:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion=crit, max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For criterion = {crit}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get the best training and validation accuracies for criterion = \"gini\". The other options give similar training accuracy but perform poorly on the validation. <br>\n",
    "So, we choose \"gini\" as the impurity measure as it's also faster to compute."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the depth of decision trees, max_depth hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth = None\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_depth = 10\n",
      "\t Training accuracy: 0.9569444444444445 \t\t Validation accuracy: 0.7530864197530864\n",
      "For max_depth = 20\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_depth = 30\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_depth = 40\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_depth = 100\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "max_depth = [None, 10, 20, 30, 40, 100]\n",
    "\n",
    "for md in max_depth:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=md, min_samples_split=2,\n",
    "                                     min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For max_depth = {md}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that the max_depth hyperparameter doesn't have any significant impact on the model's performance on both training and validation sets and we get fairly same result even with small depth of decision trees. \n",
    "- This can be explained with the fact that the dataset is pretty small with few features and also most of the features have small range of values and thus we don't need a large number of trees for convergence. <br>\n",
    "So, we choose to keep the default value of max_depth, i.e. None, in which case the nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the minimum number of samples required to split an internal node, min_samples_split hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For min_samples_split = 2\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For min_samples_split = 5\n",
      "\t Training accuracy: 0.9569444444444445 \t\t Validation accuracy: 0.5308641975308642\n",
      "For min_samples_split = 10\n",
      "\t Training accuracy: 0.9222222222222223 \t\t Validation accuracy: 0.7283950617283951\n",
      "For min_samples_split = 15\n",
      "\t Training accuracy: 0.9055555555555556 \t\t Validation accuracy: 0.654320987654321\n",
      "For min_samples_split = 20\n",
      "\t Training accuracy: 0.8847222222222222 \t\t Validation accuracy: 0.6049382716049383\n",
      "For min_samples_split = 30\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.691358024691358\n",
      "For min_samples_split = 40\n",
      "\t Training accuracy: 0.8541666666666666 \t\t Validation accuracy: 0.6419753086419753\n",
      "For min_samples_split = 50\n",
      "\t Training accuracy: 0.8513888888888889 \t\t Validation accuracy: 0.7037037037037037\n",
      "For min_samples_split = 60\n",
      "\t Training accuracy: 0.8333333333333334 \t\t Validation accuracy: 0.6049382716049383\n",
      "For min_samples_split = 70\n",
      "\t Training accuracy: 0.8375 \t\t Validation accuracy: 0.691358024691358\n",
      "For min_samples_split = 80\n",
      "\t Training accuracy: 0.8194444444444444 \t\t Validation accuracy: 0.7530864197530864\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = [2, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80]\n",
    "\n",
    "for mss in min_samples_split:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=None, min_samples_split=mss,\n",
    "                                     min_samples_leaf=1, max_features='sqrt', max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For min_samples_split = {mss}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for min_samples_split = 2. For other values, the training and validation accuracy keep oscillating but are still less than that for min_samples_split = 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the minimum number of samples required to be at leaf node, min_samples_leaf hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For min_samples_leaf = 1\n",
      "\t Training accuracy: 0.9763888888888889 \t\t Validation accuracy: 0.7777777777777778\n",
      "For min_samples_leaf = 2\n",
      "\t Training accuracy: 0.9375 \t\t Validation accuracy: 0.6666666666666666\n",
      "For min_samples_leaf = 5\n",
      "\t Training accuracy: 0.8944444444444445 \t\t Validation accuracy: 0.7901234567901234\n",
      "For min_samples_leaf = 10\n",
      "\t Training accuracy: 0.8680555555555556 \t\t Validation accuracy: 0.7901234567901234\n",
      "For min_samples_leaf = 15\n",
      "\t Training accuracy: 0.8347222222222223 \t\t Validation accuracy: 0.7777777777777778\n",
      "For min_samples_leaf = 20\n",
      "\t Training accuracy: 0.8319444444444445 \t\t Validation accuracy: 0.7530864197530864\n"
     ]
    }
   ],
   "source": [
    "min_samples_leaf = [1, 2, 5, 10, 15, 20]\n",
    "\n",
    "for msl in min_samples_leaf:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=msl, max_features='sqrt', max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For min_samples_leaf = {msl}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for min_samples_leaf = 5. For other values, the training and validation accuracy increase or decrease together but are still less than that for min_samples_leaf = 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the number of features to consider when looking for best split, max_features hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_features = 1\n",
      "\t Training accuracy: 0.85 \t\t Validation accuracy: 0.7283950617283951\n",
      "For max_features = 2\n",
      "\t Training accuracy: 0.875 \t\t Validation accuracy: 0.7530864197530864\n",
      "For max_features = 3\n",
      "\t Training accuracy: 0.8944444444444445 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_features = 4\n",
      "\t Training accuracy: 0.8888888888888888 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_features = 5\n",
      "\t Training accuracy: 0.9069444444444444 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_features = 6\n",
      "\t Training accuracy: 0.9041666666666667 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_features = 7\n",
      "\t Training accuracy: 0.8986111111111111 \t\t Validation accuracy: 0.6172839506172839\n",
      "For max_features = 8\n",
      "\t Training accuracy: 0.8986111111111111 \t\t Validation accuracy: 0.7037037037037037\n",
      "For max_features = 9\n",
      "\t Training accuracy: 0.8972222222222223 \t\t Validation accuracy: 0.7530864197530864\n",
      "For max_features = 10\n",
      "\t Training accuracy: 0.8986111111111111 \t\t Validation accuracy: 0.6790123456790124\n",
      "For max_features = 11\n",
      "\t Training accuracy: 0.9055555555555556 \t\t Validation accuracy: 0.5432098765432098\n",
      "For max_features = 12\n",
      "\t Training accuracy: 0.9055555555555556 \t\t Validation accuracy: 0.5432098765432098\n"
     ]
    }
   ],
   "source": [
    "max_features = [i+1 for i in range(12)]\n",
    "\n",
    "for mf in max_features:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=5, max_features=mf, max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For max_features = {mf}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for max_features = 5. \n",
    "- For max_features < 5, both the training and validation accuracy are less than that for max_features = 5.\n",
    "- For max_features > 5, the training accuracy is pretty close to that of max_features = 5 but the validation accuracy oscillates and stays lower than that for max_features = 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the maximum number of leaf nodes, max_leaf_nodes hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_leaf_nodes = None\n",
      "\t Training accuracy: 0.9069444444444444 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_leaf_nodes = 2\n",
      "\t Training accuracy: 0.7472222222222222 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_leaf_nodes = 3\n",
      "\t Training accuracy: 0.8 \t\t Validation accuracy: 0.8148148148148148\n",
      "For max_leaf_nodes = 4\n",
      "\t Training accuracy: 0.7986111111111112 \t\t Validation accuracy: 0.7901234567901234\n",
      "For max_leaf_nodes = 5\n",
      "\t Training accuracy: 0.8027777777777778 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_leaf_nodes = 6\n",
      "\t Training accuracy: 0.825 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_leaf_nodes = 7\n",
      "\t Training accuracy: 0.8277777777777777 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_leaf_nodes = 8\n",
      "\t Training accuracy: 0.8430555555555556 \t\t Validation accuracy: 0.8148148148148148\n",
      "For max_leaf_nodes = 9\n",
      "\t Training accuracy: 0.8541666666666666 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_leaf_nodes = 10\n",
      "\t Training accuracy: 0.8555555555555555 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_leaf_nodes = 11\n",
      "\t Training accuracy: 0.8555555555555555 \t\t Validation accuracy: 0.7777777777777778\n",
      "For max_leaf_nodes = 12\n",
      "\t Training accuracy: 0.8569444444444444 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_leaf_nodes = 13\n",
      "\t Training accuracy: 0.8569444444444444 \t\t Validation accuracy: 0.8024691358024691\n",
      "For max_leaf_nodes = 14\n",
      "\t Training accuracy: 0.8736111111111111 \t\t Validation accuracy: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "max_leaf_nodes = [None]\n",
    "max_leaf_nodes.extend([i for i in range(2,15)])\n",
    "\n",
    "\n",
    "for mln in max_leaf_nodes:\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=5, max_features=5, max_leaf_nodes=mln, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    acc_train = rnd_clf.score(x_train, y_train)\n",
    "    acc_val = rnd_clf.score(x_val, y_val)\n",
    "    print(f'For max_leaf_nodes = {mln}')\n",
    "    print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get best training and validation accuracies for max_leaf_nodes = None. \n",
    "- For other values also, the training and validation accuracies are close to that of max_leaf_nodes = None but the model may not generalize well on new data so we'll keep it as None, which means unlimited number of leaf nodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Random Forest Classifier with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: 0.9069444444444444 \t\t Validation accuracy: 0.8024691358024691\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=6, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                     min_samples_leaf=5, max_features=5, max_leaf_nodes=None, n_jobs=-1,\n",
    "                                     random_state=1001)\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "acc_train = rnd_clf.score(x_train, y_train)\n",
    "acc_val = rnd_clf.score(x_val, y_val)\n",
    "print(f'\\t Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9069444444444444 \t\t Validation accuracy: 0.8024691358024691\n",
      "Training precision score: 0.906184306401226 \t\t Validation precision score: 0.7909677419354839\n",
      "Training recall score: 0.8957598903251077 \t\t Validation recall score: 0.7909677419354839\n",
      "Training f1 score: 0.9003666029844581 \t\t Validation f1 score: 0.7909677419354838\n"
     ]
    }
   ],
   "source": [
    "acc_train = rnd_clf.score(x_train, y_train)\n",
    "acc_val = rnd_clf.score(x_val, y_val)\n",
    "\n",
    "y_train_pred = rnd_clf.predict(x_train)\n",
    "y_val_pred = rnd_clf.predict(x_val)\n",
    "\n",
    "prec_train = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "prec_val = precision_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "rec_train = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "rec_val = recall_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "f1_train = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "print(f'Training accuracy: {acc_train} \\t\\t Validation accuracy: {acc_val}')\n",
    "print(f'Training precision score: {prec_train} \\t\\t Validation precision score: {prec_val}')\n",
    "print(f'Training recall score: {rec_train} \\t\\t Validation recall score: {rec_val}')\n",
    "print(f'Training f1 score: {f1_train} \\t\\t Validation f1 score: {f1_val}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex            :  0.45155244326816407\n",
      "Fare           :  0.13349421760078592\n",
      "Cabin          :  0.10841338297023063\n",
      "Ticket         :  0.07298408085036777\n",
      "Pclass         :  0.06553827306839469\n",
      "Name           :  0.058874655078237505\n",
      "Age            :  0.038738411325067425\n",
      "Parch          :  0.032899678278285854\n",
      "Embarked       :  0.01788487501453154\n",
      "PassengerId    :  0.010683281517824208\n",
      "SibSp          :  0.00893670102811036\n"
     ]
    }
   ],
   "source": [
    "for name, score in sorted(zip(train.drop('Survived', axis=1).columns, rnd_clf.feature_importances_), \n",
    "                          reverse=True, key=lambda x: x[1]):\n",
    "    print(f'{name:<15}:  {score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that Sex was one of the most important features in the survival of passengers along with few other features, viz. Fare, Cabin, Ticket, Pclass, Name and Age.\n",
    "- Sex & Age: Women and children were given priority to leave on the lifeboat.\n",
    "- Fare, Cabin, Ticket & Pclass: The rich people had paid higher fare and had higher ticket class(Pclass). Some passengers were trapped in the flooded cabin and didn't survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[419,  25],\n",
       "       [ 42, 234]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42,  8],\n",
       "       [ 8, 23]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx = confusion_matrix(y_val, y_val_pred)\n",
    "conf_mx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;log_clf&#x27;, LogisticRegression(C=0.01)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             (&#x27;rnd_clf&#x27;,\n",
       "                              RandomForestClassifier(max_features=5,\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;log_clf&#x27;, LogisticRegression(C=0.01)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             (&#x27;rnd_clf&#x27;,\n",
       "                              RandomForestClassifier(max_features=5,\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1, voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>log_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000, probability=True,\n",
       "    random_state=1331)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rnd_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=5, min_samples_leaf=5, n_estimators=6,\n",
       "                       n_jobs=-1, random_state=1001)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('log_clf', LogisticRegression(C=0.01)),\n",
       "                             ('svm_clf',\n",
       "                              SVC(C=10, coef0=0, kernel='poly', max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             ('rnd_clf',\n",
       "                              RandomForestClassifier(max_features=5,\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('log_clf', softmax_reg), ('svm_clf', svm_clf), ('rnd_clf', rnd_clf)],\n",
    "    voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "\t Validation accuracy: 0.8024691358024691\n",
      "SVC\n",
      "\t Validation accuracy: 0.8148148148148148\n",
      "RandomForestClassifier\n",
      "\t Validation accuracy: 0.8024691358024691\n",
      "VotingClassifier\n",
      "\t Validation accuracy: 0.8333476907901234\n"
     ]
    }
   ],
   "source": [
    "for clf in (softmax_reg, svm_clf, rnd_clf, ensemble_clf):\n",
    "    acc_val = clf.score(x_val, y_val)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(f'\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "\t Test accuracy: 0.8\n",
      "SVC\n",
      "\t Test accuracy: 0.7444444444444445\n",
      "RandomForestClassifier\n",
      "\t Test accuracy: 0.8222222222222222\n",
      "VotingClassifier\n",
      "\t Test accuracy: 0.8432242347234834\n"
     ]
    }
   ],
   "source": [
    "for clf in (softmax_reg, svm_clf, rnd_clf, ensemble_clf):\n",
    "    acc_test = clf.score(x_test, y_test)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(f'\\t Test accuracy: {acc_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;log_clf&#x27;, LogisticRegression(C=0.01)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             (&#x27;rnd_clf&#x27;,\n",
       "                              RandomForestClassifier(min_samples_leaf=20,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;log_clf&#x27;, LogisticRegression(C=0.01)),\n",
       "                             (&#x27;svm_clf&#x27;,\n",
       "                              SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             (&#x27;rnd_clf&#x27;,\n",
       "                              RandomForestClassifier(min_samples_leaf=20,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>log_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, coef0=0, kernel=&#x27;poly&#x27;, max_iter=1000, probability=True,\n",
       "    random_state=1331)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rnd_clf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(min_samples_leaf=20, n_estimators=6, n_jobs=-1,\n",
       "                       random_state=1001)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('log_clf', LogisticRegression(C=0.01)),\n",
       "                             ('svm_clf',\n",
       "                              SVC(C=10, coef0=0, kernel='poly', max_iter=1000,\n",
       "                                  probability=True, random_state=1331)),\n",
       "                             ('rnd_clf',\n",
       "                              RandomForestClassifier(min_samples_leaf=20,\n",
       "                                                     n_estimators=6, n_jobs=-1,\n",
       "                                                     random_state=1001))],\n",
       "                 n_jobs=-1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[('log_clf', softmax_reg), ('svm_clf', svm_clf), ('rnd_clf', rnd_clf)],\n",
    "    voting='hard', n_jobs=-1)\n",
    "\n",
    "ensemble_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "\t Validation accuracy: 0.8024691358024691\n",
      "SVC\n",
      "\t Validation accuracy: 0.8148148148148148\n",
      "RandomForestClassifier\n",
      "\t Validation accuracy: 0.7530864197530864\n",
      "VotingClassifier\n",
      "\t Validation accuracy: 0.8210020117777778\n"
     ]
    }
   ],
   "source": [
    "for clf in (softmax_reg, svm_clf, rnd_clf, ensemble_clf):\n",
    "    acc_val = clf.score(x_val, y_val)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(f'\\t Validation accuracy: {acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "\t Test accuracy: 0.8\n",
      "SVC\n",
      "\t Test accuracy: 0.7444444444444445\n",
      "RandomForestClassifier\n",
      "\t Test accuracy: 0.8\n",
      "VotingClassifier\n",
      "\t Test accuracy: 0.8232242347234835\n"
     ]
    }
   ],
   "source": [
    "for clf in (softmax_reg, svm_clf, rnd_clf, ensemble_clf):\n",
    "    acc_test = clf.score(x_test, y_test)\n",
    "    print(clf.__class__.__name__)\n",
    "    print(f'\\t Test accuracy: {acc_test}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above validation and test accuracy scores, we can see that the ensemble outperforms each individual classifier and proves that ensembling boosts the performance by grouping several weak learners to classify the instances. <br>\n",
    "- Also, the soft voting gives better accuracy as now the ensemble predicts the class with the\n",
    "highest class probability, averaged over all the individual classifiers as compared to hard voting that predicts the class by aggregating the predictions of each classifier and predicts the class that gets the majority votes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
